{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loisll/MMAI984/blob/main/lois_trainmodel_Team_Project_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 1: Import Libraries"
      ],
      "metadata": {
        "id": "6tFQaOxS6kEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: Import libraries\n",
        "!pip install torch torchvision transformers\n",
        "!pip install tqdm\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "2jO3Rar8UkF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db37492b-bf42-4c43-8e7c-cf641bc8994a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data path\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/'\n"
      ],
      "metadata": {
        "id": "0aSiwO_X7Gdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9c43bd-3a60-480c-b9c3-62023809717a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y91qSbw0mgDO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 2: LOAD TRAIN DATA"
      ],
      "metadata": {
        "id": "MrIfrgQt6qCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for loading questions and annotations\n",
        "\n",
        "def load_data(data_file, feature):\n",
        "\n",
        "  # Check if the file exists\n",
        "  if os.path.exists(data_file):\n",
        "    print(\"File found:\", data_file)\n",
        "\n",
        "    # Load the JSON file using the json module\n",
        "    with open(data_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert the JSON data to a DataFrame\n",
        "    # questions = pd.DataFrame(data)\n",
        "\n",
        "    # Flatten the JSON structure\n",
        "    data = pd.json_normalize(data[feature])\n",
        "\n",
        "    # Question preprocessing\n",
        "\n",
        "    print(\"Data loaded successfully\")\n",
        "  else:\n",
        "    print(\"File not found:\", data_file)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "9u2juAo2b-h9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training questions\n",
        "\n",
        "# Define the questions file\n",
        "train_questions_file = os.path.join(data_path, 'train2015/MultipleChoice_abstract_v002_train2015_questions.json')\n",
        "#train_questions_file = os.path.join(data_path)\n",
        "train_questions_feature = 'questions'\n",
        "train_questions = load_data(train_questions_file, train_questions_feature )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the annotations file\n",
        "train_annotations_file = os.path.join(data_path, 'train2015/abstract_v002_train2015_annotations.json')\n",
        "train_annotations_feature = 'annotations'\n",
        "train_annotations = load_data(train_annotations_file, train_annotations_feature)\n",
        "\n",
        "\n",
        "\n",
        "# Define the annotations file\n",
        "train_captions_file = os.path.join(data_path, 'train2015/captions_abstract_v002_train2015.json')\n",
        "train_captions_feature = 'images'\n",
        "train_captions = load_data(train_captions_file, train_captions_feature)\n",
        "\n",
        "\n",
        "\n",
        "# Define the annotations file\n",
        "train_OpenEnded_file = os.path.join(data_path, 'train2015/OpenEnded_abstract_v002_train2015_questions.json')\n",
        "train_OpenEnded_feature = 'questions'\n",
        "train_OpenEnded = load_data(train_OpenEnded_file, train_OpenEnded_feature)"
      ],
      "metadata": {
        "id": "tS6fZlUDdtlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c77682-7672-4e3c-d4c2-3fc1179fdecc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found: /content/drive/My Drive/Colab Notebooks/train2015/MultipleChoice_abstract_v002_train2015_questions.json\n",
            "Data loaded successfully\n",
            "File found: /content/drive/My Drive/Colab Notebooks/train2015/abstract_v002_train2015_annotations.json\n",
            "Data loaded successfully\n",
            "File found: /content/drive/My Drive/Colab Notebooks/train2015/captions_abstract_v002_train2015.json\n",
            "Data loaded successfully\n",
            "File found: /content/drive/My Drive/Colab Notebooks/train2015/OpenEnded_abstract_v002_train2015_questions.json\n",
            "Data loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 MERGE TRAIN DATA"
      ],
      "metadata": {
        "id": "MCHBaOrZ63z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge questions and answers\n",
        "\n",
        "#train_data = pd.merge(train_questions, train_annotations, on='question_id')\n",
        "train_data = pd.merge(train_questions, train_annotations, on=[\"image_id\", \"question_id\"])\n",
        "\n",
        "df_train = train_data.merge(train_captions, on='image_id')\n",
        "df_train.head(5)"
      ],
      "metadata": {
        "id": "qo1I9qDYS_eh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "9c100bf0-0629-4ffb-d5c5-e1867b98c7d3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image_id                     question  \\\n",
              "0     11779           Who looks happier?   \n",
              "1     11779  Where is the woman sitting?   \n",
              "2     11779    Where is the man sitting?   \n",
              "3      5536          Is this man hungry?   \n",
              "4      5536  What kind of drink is that?   \n",
              "\n",
              "                                    multiple_choices  question_id  \\\n",
              "0  [alive, 1, woman, purple, 2, yes, white, boy, ...       117792   \n",
              "1  [3, no, blue, red, 1, slide, monkey bars, jump...       117790   \n",
              "2  [away, yes, blue, 1, 2, mouse, couch, no, yell...       117791   \n",
              "3  [water, yellow, 4, running, blue, pouring, out...        55360   \n",
              "4  [wine, girl would fall, soda, white, yes, coke...        55361   \n",
              "\n",
              "  question_type multiple_choice_answer  \\\n",
              "0           who                    man   \n",
              "1  where is the                blanket   \n",
              "2  where is the                  bench   \n",
              "3       is this                    yes   \n",
              "4  what kind of                   soda   \n",
              "\n",
              "                                             answers answer_type  \\\n",
              "0  [{'answer': 'old person', 'answer_confidence':...       other   \n",
              "1  [{'answer': 'on blanket', 'answer_confidence':...       other   \n",
              "2  [{'answer': 'on bench', 'answer_confidence': '...       other   \n",
              "3  [{'answer': 'yes', 'answer_confidence': 'yes',...      yes/no   \n",
              "4  [{'answer': 'water', 'answer_confidence': 'no'...       other   \n",
              "\n",
              "                                                 url  \\\n",
              "0  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "1  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "2  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "3  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "4  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "\n",
              "                                  file_name  width  height  \n",
              "0  abstract_v002_train2015_000000011779.png    700     400  \n",
              "1  abstract_v002_train2015_000000011779.png    700     400  \n",
              "2  abstract_v002_train2015_000000011779.png    700     400  \n",
              "3  abstract_v002_train2015_000000005536.png    700     400  \n",
              "4  abstract_v002_train2015_000000005536.png    700     400  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-231cddb8-ed2f-4e6d-b394-fd8e6aa02b15\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>question</th>\n",
              "      <th>multiple_choices</th>\n",
              "      <th>question_id</th>\n",
              "      <th>question_type</th>\n",
              "      <th>multiple_choice_answer</th>\n",
              "      <th>answers</th>\n",
              "      <th>answer_type</th>\n",
              "      <th>url</th>\n",
              "      <th>file_name</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11779</td>\n",
              "      <td>Who looks happier?</td>\n",
              "      <td>[alive, 1, woman, purple, 2, yes, white, boy, ...</td>\n",
              "      <td>117792</td>\n",
              "      <td>who</td>\n",
              "      <td>man</td>\n",
              "      <td>[{'answer': 'old person', 'answer_confidence':...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11779</td>\n",
              "      <td>Where is the woman sitting?</td>\n",
              "      <td>[3, no, blue, red, 1, slide, monkey bars, jump...</td>\n",
              "      <td>117790</td>\n",
              "      <td>where is the</td>\n",
              "      <td>blanket</td>\n",
              "      <td>[{'answer': 'on blanket', 'answer_confidence':...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11779</td>\n",
              "      <td>Where is the man sitting?</td>\n",
              "      <td>[away, yes, blue, 1, 2, mouse, couch, no, yell...</td>\n",
              "      <td>117791</td>\n",
              "      <td>where is the</td>\n",
              "      <td>bench</td>\n",
              "      <td>[{'answer': 'on bench', 'answer_confidence': '...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5536</td>\n",
              "      <td>Is this man hungry?</td>\n",
              "      <td>[water, yellow, 4, running, blue, pouring, out...</td>\n",
              "      <td>55360</td>\n",
              "      <td>is this</td>\n",
              "      <td>yes</td>\n",
              "      <td>[{'answer': 'yes', 'answer_confidence': 'yes',...</td>\n",
              "      <td>yes/no</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000005536.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5536</td>\n",
              "      <td>What kind of drink is that?</td>\n",
              "      <td>[wine, girl would fall, soda, white, yes, coke...</td>\n",
              "      <td>55361</td>\n",
              "      <td>what kind of</td>\n",
              "      <td>soda</td>\n",
              "      <td>[{'answer': 'water', 'answer_confidence': 'no'...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000005536.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-231cddb8-ed2f-4e6d-b394-fd8e6aa02b15')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-231cddb8-ed2f-4e6d-b394-fd8e6aa02b15 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-231cddb8-ed2f-4e6d-b394-fd8e6aa02b15');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3465ebfb-d234-4689-aaa2-9495383ae89e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3465ebfb-d234-4689-aaa2-9495383ae89e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3465ebfb-d234-4689-aaa2-9495383ae89e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 60000,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5773,\n        \"min\": 0,\n        \"max\": 19999,\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          15209,\n          1943,\n          3582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34841,\n        \"samples\": [\n          \"Is there a doll in the doll house?\",\n          \"Are the curtains exactly the same?\",\n          \"Does this scene take place in summer?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"multiple_choices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57735,\n        \"min\": 0,\n        \"max\": 199992,\n        \"num_unique_values\": 60000,\n        \"samples\": [\n          168601,\n          5252,\n          116061\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 81,\n        \"samples\": [\n          \"is the old man\",\n          \"who\",\n          \"is there a\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"multiple_choice_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2521,\n        \"samples\": [\n          \"playing soccer\",\n          \"she fell\",\n          \"sitting on floor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"other\",\n          \"yes/no\",\n          \"number\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/15209.png\",\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/1943.png\",\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/3582.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"abstract_v002_train2015_000000015209.png\",\n          \"abstract_v002_train2015_000000001943.png\",\n          \"abstract_v002_train2015_000000003582.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 700,\n        \"max\": 700,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          700\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 400,\n        \"max\": 400,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          400\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0du6UDY_S_Si"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VWVOS4oXS-25"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3 : EDA"
      ],
      "metadata": {
        "id": "ye92cJlTbbJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above examples, we can see that most of the questions and answers are simple and clean text but some questions contain punctuation, common word contractions like what’s, it’s, they’re, etc, and noun contractions like guy’s, man’s, dog’s, etc., and some answers also contain punctuation. Hence, we need to perform the data cleaning operation on the question and answer dataset and expand contractions before performing EDA."
      ],
      "metadata": {
        "id": "VdirF7O_apPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decontractions(phrase):\n",
        "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
        "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"he\\'s\", \"he is\", phrase)\n",
        "    phrase = re.sub(r\"she\\'s\", \"she is\", phrase)\n",
        "    phrase = re.sub(r\"it\\'s\", \"it is\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"he\\’s\", \"he is\", phrase)\n",
        "    phrase = re.sub(r\"she\\’s\", \"she is\", phrase)\n",
        "    phrase = re.sub(r\"it\\’s\", \"it is\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
        "\n",
        "    return phrase\n",
        "\n",
        "\n",
        "def text_preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = decontractions(text) # replace contractions into natural form\n",
        "    text = re.sub('[-,:]', ' ', text) # replace the character \"-\" \",\" with space\n",
        "    text = re.sub(\"(?!<=\\d)(\\.)(?!\\d)\", '', text) # remove the character \".\", except from floating numbers\n",
        "    text = re.sub('[^A-Za-z0-9. ]+', '', text) # remove all punctuation, except A-Za-z0-9\n",
        "    text = re.sub(' +', ' ', text) # remove extra space\n",
        "    return text\n",
        "\n",
        "# Question and Answer text preprocessing\n",
        "df_train[\"question_preprocessed\"] = df_train[\"question\"].map(lambda x: text_preprocess(x))\n",
        "df_train[\"answer_preprocessed\"] = df_train[\"multiple_choice_answer\"].map(lambda x: text_preprocess(x))"
      ],
      "metadata": {
        "id": "qhNOwuXM7qqa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "HJNyfyZ47qkw",
        "outputId": "ba84a4f7-758d-4e04-bdcf-5f7c4723dc50"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image_id                     question  \\\n",
              "0     11779           Who looks happier?   \n",
              "1     11779  Where is the woman sitting?   \n",
              "2     11779    Where is the man sitting?   \n",
              "\n",
              "                                    multiple_choices  question_id  \\\n",
              "0  [alive, 1, woman, purple, 2, yes, white, boy, ...       117792   \n",
              "1  [3, no, blue, red, 1, slide, monkey bars, jump...       117790   \n",
              "2  [away, yes, blue, 1, 2, mouse, couch, no, yell...       117791   \n",
              "\n",
              "  question_type multiple_choice_answer  \\\n",
              "0           who                    man   \n",
              "1  where is the                blanket   \n",
              "2  where is the                  bench   \n",
              "\n",
              "                                             answers answer_type  \\\n",
              "0  [{'answer': 'old person', 'answer_confidence':...       other   \n",
              "1  [{'answer': 'on blanket', 'answer_confidence':...       other   \n",
              "2  [{'answer': 'on bench', 'answer_confidence': '...       other   \n",
              "\n",
              "                                                 url  \\\n",
              "0  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "1  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "2  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "\n",
              "                                  file_name  width  height  \\\n",
              "0  abstract_v002_train2015_000000011779.png    700     400   \n",
              "1  abstract_v002_train2015_000000011779.png    700     400   \n",
              "2  abstract_v002_train2015_000000011779.png    700     400   \n",
              "\n",
              "        question_preprocessed answer_preprocessed  \n",
              "0           who looks happier                 man  \n",
              "1  where is the woman sitting             blanket  \n",
              "2    where is the man sitting               bench  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4caae1e8-75c9-4c97-a5a0-c3ea87a85ffc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>question</th>\n",
              "      <th>multiple_choices</th>\n",
              "      <th>question_id</th>\n",
              "      <th>question_type</th>\n",
              "      <th>multiple_choice_answer</th>\n",
              "      <th>answers</th>\n",
              "      <th>answer_type</th>\n",
              "      <th>url</th>\n",
              "      <th>file_name</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>question_preprocessed</th>\n",
              "      <th>answer_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11779</td>\n",
              "      <td>Who looks happier?</td>\n",
              "      <td>[alive, 1, woman, purple, 2, yes, white, boy, ...</td>\n",
              "      <td>117792</td>\n",
              "      <td>who</td>\n",
              "      <td>man</td>\n",
              "      <td>[{'answer': 'old person', 'answer_confidence':...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "      <td>who looks happier</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11779</td>\n",
              "      <td>Where is the woman sitting?</td>\n",
              "      <td>[3, no, blue, red, 1, slide, monkey bars, jump...</td>\n",
              "      <td>117790</td>\n",
              "      <td>where is the</td>\n",
              "      <td>blanket</td>\n",
              "      <td>[{'answer': 'on blanket', 'answer_confidence':...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "      <td>where is the woman sitting</td>\n",
              "      <td>blanket</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11779</td>\n",
              "      <td>Where is the man sitting?</td>\n",
              "      <td>[away, yes, blue, 1, 2, mouse, couch, no, yell...</td>\n",
              "      <td>117791</td>\n",
              "      <td>where is the</td>\n",
              "      <td>bench</td>\n",
              "      <td>[{'answer': 'on bench', 'answer_confidence': '...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "      <td>where is the man sitting</td>\n",
              "      <td>bench</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4caae1e8-75c9-4c97-a5a0-c3ea87a85ffc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4caae1e8-75c9-4c97-a5a0-c3ea87a85ffc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4caae1e8-75c9-4c97-a5a0-c3ea87a85ffc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eb4fe8d1-c6cf-44af-b379-12ee44927e7b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eb4fe8d1-c6cf-44af-b379-12ee44927e7b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eb4fe8d1-c6cf-44af-b379-12ee44927e7b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 60000,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5773,\n        \"min\": 0,\n        \"max\": 19999,\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          15209,\n          1943,\n          3582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34841,\n        \"samples\": [\n          \"Is there a doll in the doll house?\",\n          \"Are the curtains exactly the same?\",\n          \"Does this scene take place in summer?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"multiple_choices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57735,\n        \"min\": 0,\n        \"max\": 199992,\n        \"num_unique_values\": 60000,\n        \"samples\": [\n          168601,\n          5252,\n          116061\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 81,\n        \"samples\": [\n          \"is the old man\",\n          \"who\",\n          \"is there a\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"multiple_choice_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2521,\n        \"samples\": [\n          \"playing soccer\",\n          \"she fell\",\n          \"sitting on floor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"other\",\n          \"yes/no\",\n          \"number\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/15209.png\",\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/1943.png\",\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/3582.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"abstract_v002_train2015_000000015209.png\",\n          \"abstract_v002_train2015_000000001943.png\",\n          \"abstract_v002_train2015_000000003582.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 700,\n        \"max\": 700,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          700\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 400,\n        \"max\": 400,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          400\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34688,\n        \"samples\": [\n          \"how many rectangles do you count in the smaller picture on the wall\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2508,\n        \"samples\": [\n          \"he is happy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zeoi1N_e1WPZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4 : Preprocess the Image--CNN"
      ],
      "metadata": {
        "id": "D77SM2d91XNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1. Encoding image size (batch size, channel, height, weight)\n"
      ],
      "metadata": {
        "id": "_lHagUWBz33l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "# 2. 加载图像并进行预处理\n",
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    count_id = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.png'):  # only load PNG image\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image = Image.open(image_path).convert('RGB')  # open and convert to  RGB\n",
        "            image_tensor = preprocess(image)  # preprocssing as  Tensor\n",
        "            images.append(image_tensor)\n",
        "            count_id.append(filename.split('.')[0])\n",
        "    return  torch.stack(images)  # 将所有 Tensor 拼接成一个批处理 (batch) 的 Tensor\n"
      ],
      "metadata": {
        "id": "6947RFalGuGT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nlp = spacy.load('en_core_web_sm')\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "extract_dir_test = os.path.join(data_path, 'train2015/train2015_images/test')\n",
        "extract_dir_val = os.path.join(data_path, 'train2015/train2015_images/val')\n",
        "\n",
        "\n",
        "image_batch = load_images_from_folder(extract_dir_test)  # 返回大小为 (6000, 3, 224, 224) 的 Tensor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYd14DVVOLN0",
        "outputId": "04562e5c-fc0f-40ce-d56b-f40d2c16e3c3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxpHSJ2yF-XY",
        "outputId": "d7d156e9-19ff-430d-a199-8f18ad66fbb2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 Create CNN model and output as fully connected Layer (batch size, channel* height * weight)"
      ],
      "metadata": {
        "id": "8X2tBJIyaCdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: CNN Model After Image Encoding\n",
        "class CNNAfterEncoding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNAfterEncoding, self).__init__()\n",
        "        # Encoder (ResNet50)\n",
        "         # First Conv Layer: 3 input channels (RGB), 16 output channels, 3x3 kernel size\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Second Conv Layer: 16 input channels, 32 output channels, 3x3 kernel size\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Max Pool Layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully Connected (hidden) layer: Flatten the input to fit the fully connected layer\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # 32 filters, 56x56 feature map size after pooling twice\n",
        "\n",
        "        # Output layer: 128 input features, 10 output classes (example for classification)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through first Conv layer followed by ReLU and Max Pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Output: (batch_size, 16, 112, 112)\n",
        "\n",
        "        # Pass through second Conv layer followed by ReLU and Max Pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Output: (batch_size, 32, 56, 56)\n",
        "\n",
        "        # Flatten the tensor for the fully connected layer\n",
        "        x = x.view(-1, 32 * 56 * 56)  # Flatten to shape (batch_size, 32 * 56 * 56)\n",
        "\n",
        "        # Pass through the fully connected hidden layer\n",
        "        x = F.relu(self.fc1(x))  # Output: (batch_size, 128)\n",
        "\n",
        "        # # Pass through the final output layer\n",
        "        # x = self.fc2(x)  # Output: (batch_size, 10) - logits for 10 classes ...we dont need output for CNN\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# Initialize model\n",
        "model = CNNAfterEncoding()\n",
        "output = model(image_batch)#final fully connected layer output\n",
        "# # Forward pass through the model\n",
        "# model.eval()  # Set the model to evaluation mode\n",
        "# with torch.no_grad():\n",
        "#     output = model(image_batch)\n",
        "print(f'Output logits: {output.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmc1x0grFJPb",
        "outputId": "c1d37c40-7932-44da-e6fd-d888b82f3595"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output logits: torch.Size([18, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5 : Preprocessing the Question --RNN"
      ],
      "metadata": {
        "id": "PvP5JYffLKvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建一个简单的词汇表，将问题文本转换为索引\n",
        "# 使用 LabelEncoder 将每个唯一单词转换为索引\n",
        "all_questions = \" \".join(df_train['question_preprocessed'][:18]).split()\n",
        "unique_words = list(set(all_questions))\n",
        "word_to_index = {word: index for index, word in enumerate(unique_words)}\n",
        "vocab_size = len(word_to_index)  # 词汇表大小\n",
        "\n",
        "# 定义 QuestionEncoder 类\n",
        "class QuestionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(QuestionEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, question):\n",
        "        embedded = self.embedding(question)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        return hidden[-1]  # 返回最后一层的隐藏状态\n",
        "\n",
        "# 模型参数\n",
        "embed_size = 128     # 嵌入维度\n",
        "hidden_size = 64     # LSTM 隐藏层维度\n",
        "num_layers = 2       # LSTM 层数\n",
        "\n",
        "# 创建模型实例\n",
        "model = QuestionEncoder(vocab_size, embed_size, hidden_size, num_layers)\n",
        "\n",
        "# 将问题文本转换为索引\n",
        "def text_to_indices(text):\n",
        "    # 将问题拆分为单词并转换为对应的索引\n",
        "    return [word_to_index[word] for word in text.split() if word in word_to_index]\n",
        "\n",
        "# 将 DataFrame 中的问题转换为索引张量\n",
        "questions_indices = [text_to_indices(question) for question in df_train['question_preprocessed'][:18]]\n",
        "# 填充索引以使其具有相同的长度（假设最大长度为10）\n",
        "max_length = 10\n",
        "questions_indices_padded = [q + [0] * (max_length - len(q)) if len(q) < max_length else q[:max_length] for q in questions_indices]\n",
        "\n",
        "# 将问题索引转换为 Tensor\n",
        "questions_tensor = torch.tensor(questions_indices_padded, dtype=torch.long)\n",
        "\n",
        "# 前向传播，获取问题特征\n",
        "question_features = model(questions_tensor)\n",
        "\n",
        "# 输出特征的形状\n",
        "print(\"Output shape:\", question_features.shape)  # 应该是 (batch_size, hidden_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_OzsJn1MiBu",
        "outputId": "323d9004-f084-4146-a62c-d7ded30c1824"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([18, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6 : Answer Encoder\n"
      ],
      "metadata": {
        "id": "pN4LNX5lrjxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers = df_train[\"answer_preprocessed\"][:18]\n",
        "\n",
        "# Step 2: Create a vocabulary of all unique words\n",
        "vocab = {word: idx for idx, word in enumerate(set(answers))}\n",
        "print(\"Vocabulary:\", vocab)\n",
        "\n",
        "# Example Output: {'blue': 0, 'yes': 1, 'no': 2, 'maybe': 3, 'red': 4, 'definitely': 5, 'sometimes': 6, 'green': 7}\n",
        "\n",
        "# Step 3: Convert each answer into its corresponding index\n",
        "answer_indices = [torch.tensor([vocab[word]]) for word in answers]\n",
        "print(\"Answer Indices (before batching):\", answer_indices)\n",
        "\n",
        "# Example Output: [tensor([1]), tensor([2]), tensor([3]), tensor([5]), tensor([1]), tensor([2]), ...]\n",
        "\n",
        "# Step 4: Pad the sequences to ensure they have the same length\n",
        "# Since these are single words, padding is not needed. But if the answers are sequences of different lengths:\n",
        "encoded_answers_tensor = torch.tensor(answer_indices)\n",
        "print(f\"Encoded answers tensor: {encoded_answers_tensor}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuS7DCy0saJA",
        "outputId": "e8ddece5-ddf9-4d7c-ed59-495f291a4eb0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'brown': 0, 'soccer ball': 1, 'mushroom': 2, 'bench': 3, 'man': 4, 'bone': 5, 'soda': 6, 'beige': 7, 'beehive': 8, 'no': 9, 'dog': 10, 'yes': 11, 'sunny': 12, 'around tree': 13, 'blanket': 14}\n",
            "Answer Indices (before batching): [tensor([4]), tensor([14]), tensor([3]), tensor([11]), tensor([6]), tensor([7]), tensor([4]), tensor([12]), tensor([9]), tensor([8]), tensor([8]), tensor([2]), tensor([13]), tensor([1]), tensor([11]), tensor([0]), tensor([10]), tensor([5])]\n",
            "Encoded answers tensor: tensor([ 4, 14,  3, 11,  6,  7,  4, 12,  9,  8,  8,  2, 13,  1, 11,  0, 10,  5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_answers_tensor.shape"
      ],
      "metadata": {
        "id": "el0EzVu3r4dg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6deef07c-a286-42c0-cd1c-a065dcc81f6f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#step 7 :  Build VQA model"
      ],
      "metadata": {
        "id": "UySZlPCSLW-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# VQA model: Combine image and question features and predict the answer\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, image_feat_size, question_feat_size, hidden_size, answer_vocab_size):\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(image_feat_size + question_feat_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, answer_vocab_size)\n",
        "\n",
        "    def forward(self, image_features, question_features):\n",
        "        combined_features = torch.cat((image_features, question_features), dim=1) # COMBINE image fully connected layer and RNN fully connected layer\n",
        "        x = torch.relu(self.fc1(combined_features))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "5iYRsCmU2now"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EPeaodz1RQ4",
        "outputId": "d4351f1c-8269-4298-c2a1-fe4ae03513dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_answers_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Prnel7u1p39",
        "outputId": "d278bb81-9b1d-4cc6-c090-fa0fbcaa7a81"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    vocab_size = 1000  # Example vocab size (should be equal to the number of unique tokens in your question dataset)\n",
        "    embed_size = 300\n",
        "    hidden_size = 512\n",
        "    num_layers = 1\n",
        "    answer_vocab_size = 100  # Example number of possible answers\n",
        "\n",
        "\n",
        "    #question_encoder = QuestionEncoder(vocab_size, embed_size, hidden_size, num_layers)\n",
        "    vqa_model = VQAModel(image_feat_size=128, question_feat_size=64, hidden_size=512, answer_vocab_size=answer_vocab_size)\n",
        "\n",
        "    #flattened_image_features = image_batch.view(batch_size, -1)\n",
        "    # Predict the answer\n",
        "    result = vqa_model(output, question_features)\n",
        "    print(\"Model output (answer logits):\", result.shape) #18,100\n",
        "\n",
        "    # You would typically apply a softmax here to get answer probabilities\n",
        "    predicted_answer = torch.argmax(result, dim=1)\n",
        "    print(\"Predicted answer index:\", predicted_answer)\n",
        "\n",
        "\n",
        "\n",
        "    # Use CrossEntropyLoss to compare logits and ground truth answers\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = criterion(result, encoded_answers_tensor)\n",
        "    print(f\"Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "m0ymCMplAsue"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVu3hwNBCAS9",
        "outputId": "6e7141c6-e4eb-495d-deea-855124025b00"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output (answer logits): torch.Size([18, 100])\n",
            "Predicted answer index: tensor([ 5, 48, 77,  5, 41, 48, 52, 52, 41, 52, 77,  5, 48,  5, 77, 52, 77,  5])\n",
            "Loss: 4.590074062347412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qIdAgq3VNgpC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}