{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loisll/MMAI984/blob/main/lois_trainmodel_Team_Project_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7mT9MTegVHKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2209c692-44ec-42e7-c475-18d6f36913b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1: Import Libraries"
      ],
      "metadata": {
        "id": "6tFQaOxS6kEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: Import libraries\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "2jO3Rar8UkF3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data path\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/'\n"
      ],
      "metadata": {
        "id": "0aSiwO_X7Gdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad41187-7667-4f52-bed4-8fb5bec3c72e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y91qSbw0mgDO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: LOAD TRAIN DATA"
      ],
      "metadata": {
        "id": "MrIfrgQt6qCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for loading questions and annotations\n",
        "\n",
        "def load_data(data_file, feature):\n",
        "\n",
        "  # Check if the file exists\n",
        "  if os.path.exists(data_file):\n",
        "    print(\"File found:\", data_file)\n",
        "\n",
        "    # Load the JSON file using the json module\n",
        "    with open(data_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert the JSON data to a DataFrame\n",
        "    # questions = pd.DataFrame(data)\n",
        "\n",
        "    # Flatten the JSON structure\n",
        "    data = pd.json_normalize(data[feature])\n",
        "\n",
        "    # Question preprocessing\n",
        "\n",
        "    print(\"Data loaded successfully\")\n",
        "  else:\n",
        "    print(\"File not found:\", data_file)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "9u2juAo2b-h9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Load and prepare the training dataset*"
      ],
      "metadata": {
        "id": "YZR2BjFg6qHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training questions\n",
        "\n",
        "# Define the questions file\n",
        "train_questions_file = os.path.join(data_path, 'train2015/MultipleChoice_abstract_v002_train2015_questions.json')\n",
        "#train_questions_file = os.path.join(data_path)\n",
        "train_questions_feature = 'questions'\n",
        "\n",
        "train_questions = load_data(train_questions_file, train_questions_feature )\n",
        "\n",
        "# Check the first 5 rows\n",
        "#train_questions"
      ],
      "metadata": {
        "id": "tS6fZlUDdtlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ae240de-9c46-42da-ed13-f3d22bbe6a83"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found: /content/drive/My Drive/Colab Notebooks/train2015/MultipleChoice_abstract_v002_train2015_questions.json\n",
            "Data loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trainning annotations\n",
        "\n",
        "# Define the annotations file\n",
        "train_annotations_file = os.path.join(data_path, 'train2015/abstract_v002_train2015_annotations.json')\n",
        "train_annotations_feature = 'annotations'\n",
        "train_annotations = load_data(train_annotations_file, train_annotations_feature)\n",
        "\n",
        "#print(train_annotations_file[annotations])\n",
        "# Check the first 5 rows\n",
        "#train_annotations"
      ],
      "metadata": {
        "id": "-Y9yRluPUAiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ab0419-8674-4ffd-f27b-f39ea8c510af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found: /content/drive/My Drive/Colab Notebooks/train2015/abstract_v002_train2015_annotations.json\n",
            "Data loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load trainning captions\n",
        "\n",
        "# Define the annotations file\n",
        "train_captions_file = os.path.join(data_path, 'train2015/captions_abstract_v002_train2015.json')\n",
        "train_captions_feature = 'images'\n",
        "train_captions = load_data(train_captions_file, train_captions_feature)\n",
        "\n",
        "#print(train_captions_file[annotations])\n",
        "# Check the first 5 rows\n",
        "#train_captions"
      ],
      "metadata": {
        "id": "0k9asTzT90d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2bc5a1e-71ee-4b07-9905-3b9d65be39a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found: /content/drive/My Drive/Colab Notebooks/train2015/captions_abstract_v002_train2015.json\n",
            "Data loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trainning captions\n",
        "\n",
        "# Define the annotations file\n",
        "train_OpenEnded_file = os.path.join(data_path, 'train2015/OpenEnded_abstract_v002_train2015_questions.json')\n",
        "train_OpenEnded_feature = 'questions'\n",
        "train_OpenEnded = load_data(train_OpenEnded_file, train_OpenEnded_feature)\n",
        "\n",
        "#print(train_captions_file[annotations])\n",
        "# Check the first 5 rows\n",
        "#train_OpenEnded"
      ],
      "metadata": {
        "id": "m5oG2xwr90K3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d6cedff-56d4-4eb5-b23c-e9bccd246ec5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found: /content/drive/My Drive/Colab Notebooks/train2015/OpenEnded_abstract_v002_train2015_questions.json\n",
            "Data loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 MERGE TRAIN DATA"
      ],
      "metadata": {
        "id": "MCHBaOrZ63z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge questions and answers\n",
        "\n",
        "#train_data = pd.merge(train_questions, train_annotations, on='question_id')\n",
        "train_data = pd.merge(train_questions, train_annotations, on=[\"image_id\", \"question_id\"])\n",
        "\n",
        "df_train = train_data.merge(train_captions, on='image_id')\n",
        "df_train.head(5)"
      ],
      "metadata": {
        "id": "qo1I9qDYS_eh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "7cf46f12-fbde-4d95-9015-10d47de8c766"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image_id                     question  \\\n",
              "0     11779           Who looks happier?   \n",
              "1     11779  Where is the woman sitting?   \n",
              "2     11779    Where is the man sitting?   \n",
              "3      5536          Is this man hungry?   \n",
              "4      5536  What kind of drink is that?   \n",
              "\n",
              "                                    multiple_choices  question_id  \\\n",
              "0  [alive, 1, woman, purple, 2, yes, white, boy, ...       117792   \n",
              "1  [3, no, blue, red, 1, slide, monkey bars, jump...       117790   \n",
              "2  [away, yes, blue, 1, 2, mouse, couch, no, yell...       117791   \n",
              "3  [water, yellow, 4, running, blue, pouring, out...        55360   \n",
              "4  [wine, girl would fall, soda, white, yes, coke...        55361   \n",
              "\n",
              "  question_type multiple_choice_answer  \\\n",
              "0           who                    man   \n",
              "1  where is the                blanket   \n",
              "2  where is the                  bench   \n",
              "3       is this                    yes   \n",
              "4  what kind of                   soda   \n",
              "\n",
              "                                             answers answer_type  \\\n",
              "0  [{'answer': 'old person', 'answer_confidence':...       other   \n",
              "1  [{'answer': 'on blanket', 'answer_confidence':...       other   \n",
              "2  [{'answer': 'on bench', 'answer_confidence': '...       other   \n",
              "3  [{'answer': 'yes', 'answer_confidence': 'yes',...      yes/no   \n",
              "4  [{'answer': 'water', 'answer_confidence': 'no'...       other   \n",
              "\n",
              "                                                 url  \\\n",
              "0  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "1  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "2  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "3  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "4  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "\n",
              "                                  file_name  width  height  \n",
              "0  abstract_v002_train2015_000000011779.png    700     400  \n",
              "1  abstract_v002_train2015_000000011779.png    700     400  \n",
              "2  abstract_v002_train2015_000000011779.png    700     400  \n",
              "3  abstract_v002_train2015_000000005536.png    700     400  \n",
              "4  abstract_v002_train2015_000000005536.png    700     400  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6bf95916-7468-404d-b54f-846b1747a3e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>question</th>\n",
              "      <th>multiple_choices</th>\n",
              "      <th>question_id</th>\n",
              "      <th>question_type</th>\n",
              "      <th>multiple_choice_answer</th>\n",
              "      <th>answers</th>\n",
              "      <th>answer_type</th>\n",
              "      <th>url</th>\n",
              "      <th>file_name</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11779</td>\n",
              "      <td>Who looks happier?</td>\n",
              "      <td>[alive, 1, woman, purple, 2, yes, white, boy, ...</td>\n",
              "      <td>117792</td>\n",
              "      <td>who</td>\n",
              "      <td>man</td>\n",
              "      <td>[{'answer': 'old person', 'answer_confidence':...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11779</td>\n",
              "      <td>Where is the woman sitting?</td>\n",
              "      <td>[3, no, blue, red, 1, slide, monkey bars, jump...</td>\n",
              "      <td>117790</td>\n",
              "      <td>where is the</td>\n",
              "      <td>blanket</td>\n",
              "      <td>[{'answer': 'on blanket', 'answer_confidence':...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11779</td>\n",
              "      <td>Where is the man sitting?</td>\n",
              "      <td>[away, yes, blue, 1, 2, mouse, couch, no, yell...</td>\n",
              "      <td>117791</td>\n",
              "      <td>where is the</td>\n",
              "      <td>bench</td>\n",
              "      <td>[{'answer': 'on bench', 'answer_confidence': '...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5536</td>\n",
              "      <td>Is this man hungry?</td>\n",
              "      <td>[water, yellow, 4, running, blue, pouring, out...</td>\n",
              "      <td>55360</td>\n",
              "      <td>is this</td>\n",
              "      <td>yes</td>\n",
              "      <td>[{'answer': 'yes', 'answer_confidence': 'yes',...</td>\n",
              "      <td>yes/no</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000005536.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5536</td>\n",
              "      <td>What kind of drink is that?</td>\n",
              "      <td>[wine, girl would fall, soda, white, yes, coke...</td>\n",
              "      <td>55361</td>\n",
              "      <td>what kind of</td>\n",
              "      <td>soda</td>\n",
              "      <td>[{'answer': 'water', 'answer_confidence': 'no'...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000005536.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6bf95916-7468-404d-b54f-846b1747a3e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6bf95916-7468-404d-b54f-846b1747a3e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6bf95916-7468-404d-b54f-846b1747a3e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-baab44e4-99fb-428a-b80c-acf42bb47d45\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-baab44e4-99fb-428a-b80c-acf42bb47d45')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-baab44e4-99fb-428a-b80c-acf42bb47d45 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 60000,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5773,\n        \"min\": 0,\n        \"max\": 19999,\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          15209,\n          1943,\n          3582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34841,\n        \"samples\": [\n          \"Is there a doll in the doll house?\",\n          \"Are the curtains exactly the same?\",\n          \"Does this scene take place in summer?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"multiple_choices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57735,\n        \"min\": 0,\n        \"max\": 199992,\n        \"num_unique_values\": 60000,\n        \"samples\": [\n          168601,\n          5252,\n          116061\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 81,\n        \"samples\": [\n          \"is the old man\",\n          \"who\",\n          \"is there a\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"multiple_choice_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2521,\n        \"samples\": [\n          \"playing soccer\",\n          \"she fell\",\n          \"sitting on floor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"other\",\n          \"yes/no\",\n          \"number\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/15209.png\",\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/1943.png\",\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/3582.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"abstract_v002_train2015_000000015209.png\",\n          \"abstract_v002_train2015_000000001943.png\",\n          \"abstract_v002_train2015_000000003582.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 700,\n        \"max\": 700,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          700\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 400,\n        \"max\": 400,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          400\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0du6UDY_S_Si"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hIvEYFRAS_Gj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUG_-14KBglt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VWVOS4oXS-25"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uGcq2brc_brV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4px8P6X8v8vF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HV8msTG47qx9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 3 : EDA**"
      ],
      "metadata": {
        "id": "ye92cJlTbbJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above examples, we can see that most of the questions and answers are simple and clean text but some questions contain punctuation, common word contractions like what’s, it’s, they’re, etc, and noun contractions like guy’s, man’s, dog’s, etc., and some answers also contain punctuation. Hence, we need to perform the data cleaning operation on the question and answer dataset and expand contractions before performing EDA."
      ],
      "metadata": {
        "id": "VdirF7O_apPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decontractions(phrase):\n",
        "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
        "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"he\\'s\", \"he is\", phrase)\n",
        "    phrase = re.sub(r\"she\\'s\", \"she is\", phrase)\n",
        "    phrase = re.sub(r\"it\\'s\", \"it is\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"he\\’s\", \"he is\", phrase)\n",
        "    phrase = re.sub(r\"she\\’s\", \"she is\", phrase)\n",
        "    phrase = re.sub(r\"it\\’s\", \"it is\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
        "\n",
        "    return phrase\n",
        "\n",
        "\n",
        "def text_preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = decontractions(text) # replace contractions into natural form\n",
        "    text = re.sub('[-,:]', ' ', text) # replace the character \"-\" \",\" with space\n",
        "    text = re.sub(\"(?!<=\\d)(\\.)(?!\\d)\", '', text) # remove the character \".\", except from floating numbers\n",
        "    text = re.sub('[^A-Za-z0-9. ]+', '', text) # remove all punctuation, except A-Za-z0-9\n",
        "    text = re.sub(' +', ' ', text) # remove extra space\n",
        "    return text\n",
        "\n",
        "# Question and Answer text preprocessing\n",
        "df_train[\"question_preprocessed\"] = df_train[\"question\"].map(lambda x: text_preprocess(x))\n",
        "df_train[\"answer_preprocessed\"] = df_train[\"multiple_choice_answer\"].map(lambda x: text_preprocess(x))"
      ],
      "metadata": {
        "id": "qhNOwuXM7qqa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "HJNyfyZ47qkw",
        "outputId": "99b65238-5d1e-48cf-f8f7-a1f63cf64e76"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image_id                     question  \\\n",
              "0     11779           Who looks happier?   \n",
              "1     11779  Where is the woman sitting?   \n",
              "2     11779    Where is the man sitting?   \n",
              "\n",
              "                                    multiple_choices  question_id  \\\n",
              "0  [alive, 1, woman, purple, 2, yes, white, boy, ...       117792   \n",
              "1  [3, no, blue, red, 1, slide, monkey bars, jump...       117790   \n",
              "2  [away, yes, blue, 1, 2, mouse, couch, no, yell...       117791   \n",
              "\n",
              "  question_type multiple_choice_answer  \\\n",
              "0           who                    man   \n",
              "1  where is the                blanket   \n",
              "2  where is the                  bench   \n",
              "\n",
              "                                             answers answer_type  \\\n",
              "0  [{'answer': 'old person', 'answer_confidence':...       other   \n",
              "1  [{'answer': 'on blanket', 'answer_confidence':...       other   \n",
              "2  [{'answer': 'on bench', 'answer_confidence': '...       other   \n",
              "\n",
              "                                                 url  \\\n",
              "0  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "1  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "2  http://visualqa.org/data/abstract_v002/scene_i...   \n",
              "\n",
              "                                  file_name  width  height  \\\n",
              "0  abstract_v002_train2015_000000011779.png    700     400   \n",
              "1  abstract_v002_train2015_000000011779.png    700     400   \n",
              "2  abstract_v002_train2015_000000011779.png    700     400   \n",
              "\n",
              "        question_preprocessed answer_preprocessed  \n",
              "0           who looks happier                 man  \n",
              "1  where is the woman sitting             blanket  \n",
              "2    where is the man sitting               bench  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0f47e41-c0c3-493b-b717-3c8b2882fa28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>question</th>\n",
              "      <th>multiple_choices</th>\n",
              "      <th>question_id</th>\n",
              "      <th>question_type</th>\n",
              "      <th>multiple_choice_answer</th>\n",
              "      <th>answers</th>\n",
              "      <th>answer_type</th>\n",
              "      <th>url</th>\n",
              "      <th>file_name</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>question_preprocessed</th>\n",
              "      <th>answer_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11779</td>\n",
              "      <td>Who looks happier?</td>\n",
              "      <td>[alive, 1, woman, purple, 2, yes, white, boy, ...</td>\n",
              "      <td>117792</td>\n",
              "      <td>who</td>\n",
              "      <td>man</td>\n",
              "      <td>[{'answer': 'old person', 'answer_confidence':...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "      <td>who looks happier</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11779</td>\n",
              "      <td>Where is the woman sitting?</td>\n",
              "      <td>[3, no, blue, red, 1, slide, monkey bars, jump...</td>\n",
              "      <td>117790</td>\n",
              "      <td>where is the</td>\n",
              "      <td>blanket</td>\n",
              "      <td>[{'answer': 'on blanket', 'answer_confidence':...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "      <td>where is the woman sitting</td>\n",
              "      <td>blanket</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11779</td>\n",
              "      <td>Where is the man sitting?</td>\n",
              "      <td>[away, yes, blue, 1, 2, mouse, couch, no, yell...</td>\n",
              "      <td>117791</td>\n",
              "      <td>where is the</td>\n",
              "      <td>bench</td>\n",
              "      <td>[{'answer': 'on bench', 'answer_confidence': '...</td>\n",
              "      <td>other</td>\n",
              "      <td>http://visualqa.org/data/abstract_v002/scene_i...</td>\n",
              "      <td>abstract_v002_train2015_000000011779.png</td>\n",
              "      <td>700</td>\n",
              "      <td>400</td>\n",
              "      <td>where is the man sitting</td>\n",
              "      <td>bench</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0f47e41-c0c3-493b-b717-3c8b2882fa28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0f47e41-c0c3-493b-b717-3c8b2882fa28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0f47e41-c0c3-493b-b717-3c8b2882fa28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc080a90-278e-4d43-b51a-6eef2788d69f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc080a90-278e-4d43-b51a-6eef2788d69f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc080a90-278e-4d43-b51a-6eef2788d69f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 60000,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5773,\n        \"min\": 0,\n        \"max\": 19999,\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          15209,\n          1943,\n          3582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34841,\n        \"samples\": [\n          \"Is there a doll in the doll house?\",\n          \"Are the curtains exactly the same?\",\n          \"Does this scene take place in summer?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"multiple_choices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57735,\n        \"min\": 0,\n        \"max\": 199992,\n        \"num_unique_values\": 60000,\n        \"samples\": [\n          168601,\n          5252,\n          116061\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 81,\n        \"samples\": [\n          \"is the old man\",\n          \"who\",\n          \"is there a\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"multiple_choice_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2521,\n        \"samples\": [\n          \"playing soccer\",\n          \"she fell\",\n          \"sitting on floor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"other\",\n          \"yes/no\",\n          \"number\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/15209.png\",\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/1943.png\",\n          \"http://visualqa.org/data/abstract_v002/scene_img/img/3582.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"abstract_v002_train2015_000000015209.png\",\n          \"abstract_v002_train2015_000000001943.png\",\n          \"abstract_v002_train2015_000000003582.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 700,\n        \"max\": 700,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          700\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 400,\n        \"max\": 400,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          400\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34688,\n        \"samples\": [\n          \"how many rectangles do you count in the smaller picture on the wall\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_preprocessed\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2508,\n        \"samples\": [\n          \"he is happy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zeoi1N_e1WPZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 4 : Preprocess the image--CNN"
      ],
      "metadata": {
        "id": "D77SM2d91XNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcF6Rz8qGbQh",
        "outputId": "379288be-45a6-4768-f3e1-a6381cbf6c59"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynlG0-X9nAKD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bzAE2pgIOufB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1. build function of preprocssing of image\n"
      ],
      "metadata": {
        "id": "_lHagUWBz33l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),  # 调整图像大小为 224x224\n",
        "#     transforms.ToTensor(),  # 转换为 Tensor\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
        "# ])\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "# 2. 加载图像并进行预处理\n",
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    count_id = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.png'):  # 只加载 PNG 图片\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image = Image.open(image_path).convert('RGB')  # 打开并转换为 RGB\n",
        "            image_tensor = preprocess(image)  # 预处理为 Tensor\n",
        "            images.append(image_tensor)\n",
        "            count_id.append(filename.split('.')[0])\n",
        "    return  torch.stack(images)  # 将所有 Tensor 拼接成一个批处理 (batch) 的 Tensor\n"
      ],
      "metadata": {
        "id": "6947RFalGuGT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nlp = spacy.load('en_core_web_sm')\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "extract_dir_test = os.path.join(data_path, 'train2015/train2015_images/test')\n",
        "extract_dir_val = os.path.join(data_path, 'train2015/train2015_images/val')\n",
        "\n",
        "\n",
        "image_batch = load_images_from_folder(extract_dir_test)  # 返回大小为 (6000, 3, 224, 224) 的 Tensor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYd14DVVOLN0",
        "outputId": "dba80fa2-f6e8-46f0-f05e-fa9abeea9795"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxpHSJ2yF-XY",
        "outputId": "d95e2f95-f91c-461a-fc0c-ca57e40fc1bc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Step 1: Pretrained ResNet for Image Encoding\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        # Load pretrained ResNet50\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        # Remove the final fully connected layer\n",
        "        self.encoder = nn.Sequential(*list(resnet.children())[:-2])  # Keep layers till the last conv layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)  # Output: (batch_size, 2048, 7, 7)\n",
        "\n",
        "# Step 2: CNN Model After Image Encoding\n",
        "class CNNAfterEncoding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNAfterEncoding, self).__init__()\n",
        "        # Encoder (ResNet50)\n",
        "         # First Conv Layer: 3 input channels (RGB), 16 output channels, 3x3 kernel size\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Second Conv Layer: 16 input channels, 32 output channels, 3x3 kernel size\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Max Pool Layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully Connected (hidden) layer: Flatten the input to fit the fully connected layer\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # 32 filters, 56x56 feature map size after pooling twice\n",
        "\n",
        "        # Output layer: 128 input features, 10 output classes (example for classification)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through first Conv layer followed by ReLU and Max Pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Output: (batch_size, 16, 112, 112)\n",
        "\n",
        "        # Pass through second Conv layer followed by ReLU and Max Pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Output: (batch_size, 32, 56, 56)\n",
        "\n",
        "        # Flatten the tensor for the fully connected layer\n",
        "        x = x.view(-1, 32 * 56 * 56)  # Flatten to shape (batch_size, 32 * 56 * 56)\n",
        "\n",
        "        # Pass through the fully connected hidden layer\n",
        "        x = F.relu(self.fc1(x))  # Output: (batch_size, 128)\n",
        "\n",
        "        # # Pass through the final output layer\n",
        "        # x = self.fc2(x)  # Output: (batch_size, 10) - logits for 10 classes\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# Initialize model\n",
        "model = CNNAfterEncoding()\n",
        "\n",
        "# Forward pass through the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    output = model(image_batch)\n",
        "    print(f'Output logits: {output.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmc1x0grFJPb",
        "outputId": "797c82d7-460f-4450-e2d7-c4c554cd4b9e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output logits: torch.Size([18, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# images_target = []\n",
        "# count_id_target = []\n",
        "# for filename in os.listdir(extract_dir_val):\n",
        "#      # Check if the file is a PNG image\n",
        "#      if filename.endswith('.png'):\n",
        "#        # Create the full path to the image\n",
        "#         image_path = os.path.join(extract_dir_val, filename)\n",
        "#               # Open the image\n",
        "#         image = Image.open(image_path)\n",
        "#               # preprocessing image\n",
        "#         tensor_gpu=preprocess_image(image_path)\n",
        "#         images_target.append(tensor_gpu)\n",
        "#         count_id_target.append(filename.split('.')[0])\n",
        "\n",
        "# images_target = np.array(images_target)\n",
        "# images_target = images_target.astype('float32')\n",
        "# images_target /= 255.0\n",
        "# images_target = tf.convert_to_tensor(images_target)\n"
      ],
      "metadata": {
        "id": "zl_mrfBLxBpw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 5 : preprocessing the Question and Answer --RNN"
      ],
      "metadata": {
        "id": "PvP5JYffLKvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建一个简单的词汇表，将问题文本转换为索引\n",
        "# 使用 LabelEncoder 将每个唯一单词转换为索引\n",
        "all_questions = \" \".join(df_train['question_preprocessed'][:18]).split()\n",
        "unique_words = list(set(all_questions))\n",
        "word_to_index = {word: index for index, word in enumerate(unique_words)}\n",
        "vocab_size = len(word_to_index)  # 词汇表大小\n",
        "\n",
        "# 定义 QuestionEncoder 类\n",
        "class QuestionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(QuestionEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, question):\n",
        "        embedded = self.embedding(question)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        return hidden[-1]  # 返回最后一层的隐藏状态\n",
        "\n",
        "# 模型参数\n",
        "embed_size = 128     # 嵌入维度\n",
        "hidden_size = 64     # LSTM 隐藏层维度\n",
        "num_layers = 2       # LSTM 层数\n",
        "\n",
        "# 创建模型实例\n",
        "model = QuestionEncoder(vocab_size, embed_size, hidden_size, num_layers)\n",
        "\n",
        "# 将问题文本转换为索引\n",
        "def text_to_indices(text):\n",
        "    # 将问题拆分为单词并转换为对应的索引\n",
        "    return [word_to_index[word] for word in text.split() if word in word_to_index]\n",
        "\n",
        "# 将 DataFrame 中的问题转换为索引张量\n",
        "questions_indices = [text_to_indices(question) for question in df_train['question_preprocessed'][:18]]\n",
        "# 填充索引以使其具有相同的长度（假设最大长度为10）\n",
        "max_length = 10\n",
        "questions_indices_padded = [q + [0] * (max_length - len(q)) if len(q) < max_length else q[:max_length] for q in questions_indices]\n",
        "\n",
        "# 将问题索引转换为 Tensor\n",
        "questions_tensor = torch.tensor(questions_indices_padded, dtype=torch.long)\n",
        "\n",
        "# 前向传播，获取问题特征\n",
        "question_features = model(questions_tensor)\n",
        "\n",
        "# 输出特征的形状\n",
        "print(\"Output shape:\", question_features.shape)  # 应该是 (batch_size, hidden_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_OzsJn1MiBu",
        "outputId": "d8a79447-7475-429f-ce35-6bf22379bf85"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([18, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "zlbKc260L09k"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnAzEMiBL020",
        "outputId": "bad72e00-0229-4980-b380-acad876c752d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18,)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer encoder\n"
      ],
      "metadata": {
        "id": "pN4LNX5lrjxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers = df_train[\"answer_preprocessed\"][:18]\n",
        "\n",
        "# Step 2: Create a vocabulary of all unique words\n",
        "vocab = {word: idx for idx, word in enumerate(set(answers))}\n",
        "print(\"Vocabulary:\", vocab)\n",
        "\n",
        "# Example Output: {'blue': 0, 'yes': 1, 'no': 2, 'maybe': 3, 'red': 4, 'definitely': 5, 'sometimes': 6, 'green': 7}\n",
        "\n",
        "# Step 3: Convert each answer into its corresponding index\n",
        "answer_indices = [torch.tensor([vocab[word]]) for word in answers]\n",
        "print(\"Answer Indices (before batching):\", answer_indices)\n",
        "\n",
        "# Example Output: [tensor([1]), tensor([2]), tensor([3]), tensor([5]), tensor([1]), tensor([2]), ...]\n",
        "\n",
        "# Step 4: Pad the sequences to ensure they have the same length\n",
        "# Since these are single words, padding is not needed. But if the answers are sequences of different lengths:\n",
        "padded_batch = pad_sequence(answer_indices, batch_first=True)\n",
        "print(\"Padded Batch Shape:\", padded_batch.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "LuS7DCy0saJA",
        "outputId": "f2b6a1ad-cf12-4daf-b3a0-b755964f65e0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'around tree': 0, 'blanket': 1, 'soccer ball': 2, 'beige': 3, 'yes': 4, 'bench': 5, 'mushroom': 6, 'dog': 7, 'man': 8, 'brown': 9, 'soda': 10, 'beehive': 11, 'sunny': 12, 'no': 13, 'bone': 14}\n",
            "Answer Indices (before batching): [tensor([8]), tensor([1]), tensor([5]), tensor([4]), tensor([10]), tensor([3]), tensor([8]), tensor([12]), tensor([13]), tensor([11]), tensor([11]), tensor([6]), tensor([0]), tensor([2]), tensor([4]), tensor([9]), tensor([7]), tensor([14])]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pad_sequence' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-36011c52d5e9>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Step 4: Pad the sequences to ensure they have the same length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Since these are single words, padding is not needed. But if the answers are sequences of different lengths:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpadded_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Padded Batch Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pad_sequence' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PYFvYHR0r4wU"
      },
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "el0EzVu3r4dg"
      },
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "tpgaXv51CMth"
      },
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rBptPfN87Fy3"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hlpGLL9B8v5X"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S09CNtHQ3fqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FcUkCsJw3fnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9uDMimH3fki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 5 :  build model"
      ],
      "metadata": {
        "id": "UySZlPCSLW-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCETnFXOzCMM",
        "outputId": "14096923-3057-4cd9-8f7e-2568ce4c81ad"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# VQA model: Combine image and question features and predict the answer\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, image_feat_size, question_feat_size, hidden_size, answer_vocab_size):\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(image_feat_size + question_feat_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, answer_vocab_size)\n",
        "\n",
        "    def forward(self, image_features, question_features):\n",
        "        combined_features = torch.cat((image_features, question_features), dim=1)\n",
        "        x = torch.relu(self.fc1(combined_features))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "5iYRsCmU2now"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EPeaodz1RQ4",
        "outputId": "2ad69f05-e483-4269-bfd4-066b69fafd46"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Prnel7u1p39",
        "outputId": "5bfc4fe6-89cd-4efa-fcfd-561a2cbe8a5c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    vocab_size = 1000  # Example vocab size (should be equal to the number of unique tokens in your question dataset)\n",
        "    embed_size = 300\n",
        "    hidden_size = 512\n",
        "    num_layers = 1\n",
        "    answer_vocab_size = 100  # Example number of possible answers\n",
        "\n",
        "\n",
        "    #question_encoder = QuestionEncoder(vocab_size, embed_size, hidden_size, num_layers)\n",
        "    vqa_model = VQAModel(image_feat_size=128, question_feat_size=64, hidden_size=512, answer_vocab_size=answer_vocab_size)\n",
        "\n",
        "    #flattened_image_features = image_batch.view(batch_size, -1)\n",
        "    # Predict the answer\n",
        "    result = vqa_model(output, question_features)\n",
        "    print(\"Model output (answer logits):\", output)\n",
        "\n",
        "    # You would typically apply a softmax here to get answer probabilities\n",
        "    predicted_answer = torch.argmax(output, dim=1)\n",
        "    print(\"Predicted answer index:\", predicted_answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0ymCMplAsue",
        "outputId": "ad5c95d3-2ae6-4a6e-83ea-e61956758d5b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output (answer logits): tensor([[0.0856, 0.0264, 0.0000,  ..., 0.2453, 0.0000, 0.0000],\n",
            "        [0.1309, 0.0000, 0.0000,  ..., 0.3332, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0931, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.1329, 0.0000, 0.0184,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0805, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0501, 0.0000, 0.0000]])\n",
            "Predicted answer index: tensor([ 10,  39,  92,  39,  67,  88,  39,  92,  39, 110, 109,  92, 115,  39,\n",
            "         78,  39, 119,  39])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVu3hwNBCAS9",
        "outputId": "baa1fd46-0b6b-4f91-8285-10de019dad26"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_batch = load_images_from_folder(extract_dir_test)  # 返回大小为 (6000, 3, 224, 224) 的 Tensor\n",
        "\n",
        "# 3. 定义卷积层 (Conv2D)\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "# 4. 将 6000 张图片传入卷积层\n",
        "output = conv_layer(image_batch)\n",
        "\n",
        "print(\"输出的形状:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKwUxT-pAveF",
        "outputId": "ee4f2532-b1cf-4068-f404-bb239d204264"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "输出的形状: torch.Size([18, 16, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # Convolutional layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Pooling layer\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # Second conv layer\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 120)  # Fully connected layer (flattened)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)  # Output layer (10 classes for CIFAR-10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))  # Apply conv1, relu, and pool\n",
        "        x = self.pool(torch.relu(self.conv2(x)))  # Apply conv2, relu, and pool\n",
        "        x = x.view(-1, 32 * 8 * 8)  # Flatten the feature maps into a vector\n",
        "        x = torch.relu(self.fc1(x))  # Apply fully connected layers\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # Output logits (class scores)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the CNN model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the CNN model with batch size\n",
        "for epoch in range(2):  # Train for 2 epochs (as an example)\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # Get the inputs (images) and labels from the data loader\n",
        "        lable=[8, 4, 7, 5, 8, 6, 6, 5, 1, 8, 7, 4, 1, 2, 4, 3]\n",
        "        inputs= images\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: compute output\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass: compute gradient and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print loss statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "IKl22UOT3fTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7AYfdPOnAqFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import spacy\n",
        "\n",
        "# Load the pre-trained spacy model for tokenization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "extract_dir_test = os.path.join(data_path, 'train2015/train2015_images/test')\n",
        "\n",
        "\n",
        "# Define the VQA Model\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, output_size):\n",
        "        super(VQAModel, self).__init__()\n",
        "        # Load pre-trained ResNet for image encoding\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.fc = nn.Identity()  # Remove the final classification layer\n",
        "\n",
        "        # LSTM for question encoding\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Fully connected layers to combine image and question encodings\n",
        "        self.fc1 = nn.Linear(hidden_size + 512, 512)\n",
        "        self.fc2 = nn.Linear(512, output_size)\n",
        "\n",
        "    def forward(self, image, question):\n",
        "        # Image encoding\n",
        "        img_features = self.resnet(image)\n",
        "\n",
        "        # Question encoding\n",
        "        question_embed = self.embedding(question)\n",
        "        _, (hidden, _) = self.lstm(question_embed)\n",
        "\n",
        "        # Combine image and question encodings\n",
        "       # combined = torch.cat((img_features, hidden.squeeze(0)), dim=1)\n",
        "         # 结合图像和问题的特征\n",
        "        combined = torch.cat((img_features, question_embed), dim=1)\n",
        "\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model Hyperparameters\n",
        "hidden_size = 256\n",
        "output_size = 3  # Let's say we are predicting one of 3 possible answers\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = VQAModel(vocab_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4bxaXUsH-BF",
        "outputId": "1f19637d-ecd8-4411-e587-5b024c6127a0"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Forward pass\n",
        "output = model(image_features, question_features)\n",
        "\n",
        "# Example target (the correct answer is index 1)\n",
        "target = torch.tensor([1])\n",
        "\n",
        "# Calculate loss\n",
        "loss = criterion(output, target)\n",
        "print(f'Loss: {loss.item()}')\n",
        "\n",
        "# Backpropagation and optimization\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Training step completed.\")\n"
      ],
      "metadata": {
        "id": "ayNNTWZK8lov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "9bbc9335-3451-484e-8816-9f3a37d04bfd"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [18, 512]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-861c58e42fcd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Example target (the correct answer is index 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-0e93352093f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, question)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Image encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Question encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 454\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    455\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [18, 512]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qblHrMYC8lko"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_EyMgY48lf3"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ShN8b6WBt_5w"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build answer"
      ],
      "metadata": {
        "id": "kdNag2nbuDIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 定义VQA模型\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, num_classes):\n",
        "        super(VQAModel, self).__init__()\n",
        "        # 使用预训练的ResNet处理图像\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # 移除最后的全连接层\n",
        "\n",
        "        # 定义LSTM处理文本\n",
        "        self.lstm = nn.LSTM(input_size=300, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "        # 定义全连接层用于组合图像和文本特征\n",
        "        self.fc1 = nn.Linear(hidden_size + 2048, 1024)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, image, question, lengths):\n",
        "         #提取图像特征\n",
        "        #img_features = self.resnet(image)\n",
        "        #img_features = img_features.view(img_features.size(0), -1)  # 展平图像特征\n",
        "\n",
        "        # 提取问题的特征\n",
        "        packed = pack_padded_sequence(question, lengths, batch_first=True, enforce_sorted=False)\n",
        "        _, (hn, _) = self.lstm(packed)\n",
        "        ques_features = hn[-1]\n",
        "\n",
        "        # 结合图像和问题的特征\n",
        "        combined = torch.cat((image, ques_features), dim=1)\n",
        "        #combined = torch.cat((img_features, ques_features), dim=1)\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        output = self.fc2(x)\n",
        "        return output\n",
        "\n",
        "# 假设你已经有数据集并进行预处理\n",
        "def preprocess_image(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    return image.unsqueeze(0)  # 添加batch维度\n",
        "\n",
        "# 简单的训练函数\n",
        "def train(model, data_loader, criterion, optimizer, num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for images, questions, lengths, labels in data_loader:\n",
        "            outputs = model(images, questions, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "tR1lrfaauKa3"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " combined = torch.cat((image.features, question_features), dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "VDDY08G05m2Q",
        "outputId": "fab3769c-307d-4fa8-968b-a62612bd195a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'keras._tf_keras.keras.preprocessing.image' has no attribute 'features'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-7f7990cf3a97>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras._tf_keras.keras.preprocessing.image' has no attribute 'features'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lengths = [4]\n",
        "\n",
        "# 初始化模型\n",
        "vocab_size = 1000  # 假设词汇表大小\n",
        "hidden_size = 512\n",
        "num_classes = 1000  # 假设有1000种可能的答案\n",
        "model = VQAModel(hidden_size, vocab_size, num_classes)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 训练模型\n",
        "# 假设你有一个data_loader，包含图像、问题、长度、标签等\n",
        "# train(model, data_loader, criterion, optimizer)\n",
        "\n",
        "# 模型推理\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(image_features, question_features, lengths)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(f'Predicted answer: {predicted.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "DU6JNx8i5ZPD",
        "outputId": "f4369cd7-74e2-4995-a291-ec711d1b2ffb"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 300x2048)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-e475563ef72f>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Predicted answer: {predicted.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-b895928a503a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, question, lengths)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# 提取问题的特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mques_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    918\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[1;32m    921\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[1;32m    922\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 300x2048)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0FsoOxa56LM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}